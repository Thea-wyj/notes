# 机器学习

> 什么是机器学习：机器学习就是**找一个函数**

## 多种类型的函数

### 回归 Regression

输出是一个**具体数值**的函数

应用：股票预测、自动驾驶、推荐系统

**==过拟合（Overfitting）==**：更复杂的模型往往会导致在训练数据上的数据要比测试数据好得多，这种现象就叫做过拟合

处理过拟合的情况：**==正则化 Regularization==**

可以使公式变得更加平滑 ，比较平滑的公式更不容易收到杂讯的影响，在测试的时候更容易获得好的结果

经典模型：**线性回归 Linear Regression**、**逻辑回归 Logistic Regression**

![image-20220930154236768](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220930154236768.png)

两种方法的优缺点分析（Generative V.S. Discriminative）

generative model 的优点

通过对概率分布的设想，可以减少训练需要的数据量，且对杂讯更加鲁棒，可以通过不同的数据源估计先验概率和类相关概率

### 分类 Classification

设置几个选项，会输出正确选项的函数

应用：信用评级、医学诊断、手写文字识别、人脸识别

为什么不用Regression解决？

Regression的结果好时，得出的结果不一定是符合分类的

> 若用Logistic Regression来实现，则需要进行Feature Transformation
>
> ![image-20220930092825411](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220930092825411.png)
>
> 把Logistic Regression连接起来，可以先进行Feature Transformation在进行分类（深度学习）
>
> ![image-20221006211010713](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20221006211010713.png)

具体过程：

二分类情况，类比于从两个箱子里面拿出不同颜色的小球，两个箱子，就是两个分类，小球就是样本，我们要求出某个样本属于某个类别的概率，即某个小球是从哪个箱子里拿出来的概率。

如下图，需要分别求解，从所有样本中取出分类为1的样本的概率、从所有样本中取出分类为2的样本的概率、从分类1里面挑出来 是取出的样本x的概率、从分类2里面挑出来 是取出的样本x的概率，前两个容易可得，后两个可通过生成模型来计算

**生成模型（Generative Model）**

![image-20220930094136140](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220930094136140.png)

想求从某个分类里挑出来样本x的概率，我们需要决定每个分类的特征向量，通过特征向量找出符合每个类别里面所有样本的高斯分布，这样就可以通过判断x距离高斯分布的远近，来确定，从这个分类里面挑出样本x的概率大小

**高斯分布 （Gaussian Distribution）**

得出来的值可以看作从分类1里面挑出x的概率（一般适用于特征是多维矩阵，特征是二进制的一般用伯努利分布）

若特征值的所有维度 都是独立的，这个分类器成为朴素贝叶斯分类器

![image-20220930095305185](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220930095305185.png)

要找出最适合某个分类下所有样本点的高斯分布，我们需要使用最大似然估计法，来确定使高斯分布里两个参数μ和Σ的值

**最大似然 （Maximum Likelihood）**

![image-20220930100258189](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220930100258189.png)

这里通过变化μ和Σ，来使整个高斯分布得出的概率值最大，通过求微分或者分别求出μ和Σ的值来确定

**总结三步**

- 定义一个函数（模型）
- 定义函数的好坏（最大似然估计）
- 找到最好的参数

![image-20220930115830469](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220930115830469.png)

**多分类的情况**

**Softmax**

对于每一个类别求出来的概率，先分别取e的z^n^次幂，再将这几个值加起来，最后取平均值，这样可将概率控制在0和1之间，可以看成是概率分布

![image-20221006205017428](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20221006205017428.png)

然后取和y'的交叉熵

![image-20221006205556362](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20221006205556362.png)



### 结构化学习 Structured Learning

输出有结构的结果数据 例如文件、图片等的函数

![image-20220910160526229](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910160526229.png)

## 如何找出这个函数

1. 创建一个具有未知参数（权重 w weight和 偏差 b bias）的函数（基于对应领域的知识）→ 模型 Model

2. 通过训练集定义损失函数 Loss（L(b,w),输出的值代表b与w的值设置的好与不好 ）
   ![image-20220910170702270](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910170702270.png)

   ![image-20220910170924274](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910170924274.png)

3. 最佳化 Optimization  分别确定使Loss最小的w和b 

   **梯度下降法 Gradient Descent** （局部最小不一定是全局最小）

   - 先画出 L-w 图像，随机选取一个初始点w0

   - 计算这一点w对L的微分

   - w向L减小的方向前进一步，步伐的大小取决于该点的微分和 学习率 learning rate  η（超参数 hyperparameters，自己设定）

   - 迭代更新 w值，直到自己设置的更新次数上限（超参数）或者w不再更新的时候

   ![image-20220910173138604](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910173138604.png)

### 线性模型

太简单，限制较多（Model Bias）

### 复杂模型

red curve = constant + sum of a set of Sigmoid Function

任何一个函数图像都可以看成是一个常量（y轴与图像的焦点) + 很多个不同形状的 sigmoid函数（不同的w、b、c）

![image-20220910195619722](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910195619722.png)

![image-20220910195856862](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910195856862.png)

将之前的公式代入Sigmoid方程里 得出 适用于更多复杂情况的模型

![image-20220910200720320](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910200720320.png)

用θ代表所有未知的参数

![image-20220910200857415](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910200857415.png)

Loss函数表示为L(θ)

![image-20220910201359019](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910201359019.png)

寻找θ的最优值，来达到模型的最佳化

![image-20220910201637521](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910201637521.png)

进一步，我们可以将θ分组，分别算Loss

![image-20220910201907252](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910201907252.png)

也可以通过ReLU进行模拟，Hard Sigmoid = 2 * ReLU

![image-20220910202252499](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910202252499.png)

ReLU 和 Sigmoid 统称为激活函数 Activation Function

也可以把以上激活层数的层数多做几层（超参数），使Loss更小

![image-20220910202802108](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910202802108.png)

每一层得出的一个激活函数结果 就是一个神经元 Neuron，多个神经元组成了神经网络 Neural Network，多层的激活函数就是多层神经网络 => 深度学习

![image-20220910203331082](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910203331082.png)

一些主流的多层神经网络

![image-20220910203548720](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910203548720.png)

但是多层的神经网络容易发生过拟合的情况 Overfitting

****

## 机器学习的几种方法

### 自监督学习（Self-supervised Learning）

#### 预训练模型 Pre-trained Model（Foundation Model）

 基础能力的训练（图片识别里面的 图片翻转等变形操作）

经典模型：**BERT** 具有340M的参数

#### 下游任务Downstream Tasks

→ 具体的能力训练

### 无监督学习（Unsupervised Learning）

#### 生成对抗网络 Generative Adversarial Network

![image-20220910161930137](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910161930137.png)

相关文献：

![image-20220910162058897](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910162058897.png)

### 强化学习 Reinforcement Learning（RL）

适用场景：不知道该怎么标注资料的时候

### 异常检测 Anomaly Detection

适用于没有明确结果的时候，“使机器具有说我不知道的能力”

### 可解释性人工智能 Explainable AI

不仅能做出正确的决策，并且机器还知道自己为什么做这样的决策

### 模型攻击 Model Attack

在样本上增加微小的杂讯，使结果出错

### 域自适应 Domain Adaptation

解决因为样本数据和测试数据差别较大时结果出现的误差

### 网络压缩 Network Compression

使模型缩小

### 终身学习 Life-long Learning

使同一个机器不断学习多个技术

### 元学习 Meta Learning （Few-shot learning）

学习 如何 学习

![image-20220910163731472](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220910163731472.png)

## 深度学习

### 发展历程

- 1958  感知机的提出（Perceptron） 线性模型
- 1969 感知机（线性模型）具有局限性
- 1980s 多层感知机（Multi-layer perceptron）（DNN）
- 1986 反向传播（Backpropagation）提出 超过3层的神经网络就没有更好的效果了
- 1989 发现一层的神经网络就已经效果很好了
- 2006 多层神经网络（感知机） 改名为 **深度学习** （RBM initialization）
- 2009 提出GPU跑模型 提高了深度学习模型的效率 
- 2011 逐渐在语音识别领域风靡
- 2012 在图像识别赢得了奖项 ILSVRC

### 步骤

#### 定义一个公式（神经网络）

![image-20220929151705568](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220929151705568.png)

常见的**神经网络的连接方式**

全连接前馈神经网络（Fully Connect Feedforward  Network）

![image-20220929153542233](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220929153542233.png)

![image-20220929153722020](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220929153722020.png)

==Deep = 许多的隐藏层==

一些比较有名的深度学习模型：

![image-20220929153928126](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220929153928126.png)

将矩阵带代入模型（可以用GPU计算）

![image-20220929154912513](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220929154912513.png)

具体构建起来的神经网络需要多少层，每层有多少个神经元，是需要设计的，需要具体问题具体分析的。

####  定义一个公式的好坏（或参数的好坏）

![image-20220929162944017](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220929162944017.png)

![image-20220929163129375](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220929163129375.png)

通过交叉熵 cross entropy的和，算出总损失，找出总损失最小的公式

#### 找到最好的公式（最优参数）

找出最小损失的方法：**梯度下降法** Gradient Descent

![image-20220929163315052](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220929163315052.png)

其中，**反向传播法**(backpropagation)是个效率很高的计算梯度，实现梯度下降的算法

 ![image-20220929163836094](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220929163836094.png)

##### Backpropagation

![image-20220929164622872](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220929164622872.png)

根据链式法则，可以得出，损失函数对参数w的微分可以分成两个部分，一个是每一层输出z对w的微分和总损失对z的微分，前部分，可以通过前向传播的方法，计算出最后的结果，而后部分需要通过反向传播进行计算

![image-20220929170421877](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220929170421877.png)

***为什么需要深度学习？***

同样的函数，通过更深层次的模型生成所需要的参数**小于**一层但是更宽的模型生成所需要的参数，效率也更高，更不容易Overfitting，因此，**复杂和更有规律**的模型，更适合用**深度学习**，深度学习可以做到，**用较少的参数，做到更小的误差**

![image-20221021105308679](https://gitee.com/dorothea817/pics/raw/master/image-20221021105308679.png)

## CNN（卷积神经网络）

***图像识别，计算机如何处理图像类的数据？***

一张图片可以被看成是一个**3维的tensor**，三个维度分别代表了RGB三原色，每个维度的矩阵的大小为图片的大小，每一个像素代表一个位置上的数据，然后将它拆开，拼成一个矩阵，就可以放到神经网络里进行分析了

![image-20221018194710758](https://gitee.com/dorothea817/pics/raw/master/image-20221018194710758.png)

如果把得出的**所有数据**直接放到一个**全连接**的神经网络里，巨大的数据量会**降低模型的弹性**，增加**过拟合**的风险

![image-20221018200410845](https://gitee.com/dorothea817/pics/raw/master/image-20221018200410845.png)

在识别图片上的内容时，我们往往不需要观察整张图片，而是观察图片里物体的某种特征区别去决定他是什么，所以神经网络也无需关心所有数据

![image-20221018200358391](https://gitee.com/dorothea817/pics/raw/master/image-20221018200358391.png)

CNN就给每个神经元设计了其特定的**Receptive field**，他只需要关心Receptive field的数据即可

![image-20221018200701461](https://gitee.com/dorothea817/pics/raw/master/image-20221018200701461.png)

![image-20221018202430731](https://gitee.com/dorothea817/pics/raw/master/image-20221018202430731.png)

**Receptive field**常用的设计形式：

- 会看所有的channels
- **kernel size**（Receptive field的大小） 为3×3
- 同一个Receptive field会有一组神经元去检测
- **stride**（前一个Receptive field 平移 一个 stride单位的距离，生成另一个新的Receptive field，是个超参数 ） 一般 为2，因为我们**希望Receptive field是有重叠的**，这样可以检测到两个Receptive field之间的pattern，如果Receptive field超出了图像的范围，我们一般将超出的地方补0（也可以别的值），称为**padding**![image-20221018203743215](https://gitee.com/dorothea817/pics/raw/master/image-20221018203743215.png)

***Parameter Sharing***

我们往往不能确定某一个pattern在图片上的具体位置，因此我们想要在每一个监视块内都存在神经元都能识别这个pattern，所以就引出共享参数 **Parameter Sharing** 的概念，让一部分的神经元共享权重w进行进一步的简化。

![image-20221018204357725](https://gitee.com/dorothea817/pics/raw/master/image-20221018204357725.png)

常见的Parameter Sharing

![image-20221018205317293](https://gitee.com/dorothea817/pics/raw/master/image-20221018205317293.png)

==Convolutional Layer（卷积层） = Receptive field + Parameter Sharing，使用了Convolutional Layer的神经网络，就被称为CNN（卷积神经网络），专门为图像识别设计的==

***CNN的运作流程***

设定n个Filter，每个Filter是大小为3×3×channel的tensor，然后用每一个Filter去识别图片里与他对应的pattern，识别过程如下：filter与图片里面的每一个Receptive field 进行内积，得出一个矩阵，然后找出矩阵里最能显示出这个pattern的数据，对应的Receptive field 可能就存在这个pattern，每个Filter内积得出的矩阵叠在一起就是Feature Map，Feature Map的channel数和filters的数量一样

![image-20221018212320925](https://gitee.com/dorothea817/pics/raw/master/image-20221018212320925.png)

多层的CN就把上一层得出的Feature Map看作图片，再进行一层卷积

![image-20221018213114850](https://gitee.com/dorothea817/pics/raw/master/image-20221018213114850.png)

filter就是神经元对应的权重（也有Bias），共享参数实际上就是一个神经元的filter扫过weight，也就是**卷积操作**

![image-20221018213531093](https://gitee.com/dorothea817/pics/raw/master/image-20221018213531093.png)

***Pooling （池化）***

把图片变小，比**如最大池化**就是把filter 产生的feature map 分组，每一组选一个最大的值，然后把其他的值都去掉，进行缩小的操作

![image-20221018213810093](https://gitee.com/dorothea817/pics/raw/master/image-20221018213810093.png)

![image-20221018214510128](https://gitee.com/dorothea817/pics/raw/master/image-20221018214510128.png)

但是CNN并不能和处理图像放大、缩小、旋转的问题，可以增加Data Augmentation（数据增强）过程来实现

### Spatial Transformer Layer

可以通过在CNN之前加一个空间变换层，来处理图像，使之放大、缩小、旋转、翻转等，来解决CNN无法处理类似图像的问题

![image-20221021142809138](https://gitee.com/dorothea817/pics/raw/master/image-20221021142809138.png)

这个空间变换层其实用一个全连接的神经网络中前一层对后一层的映射就可以实现，但是这个过程是无法微分的，没办法用梯度下降法去训练

![image-20221021145531843](https://gitee.com/dorothea817/pics/raw/master/image-20221021145531843.png)

这里引入Interpolation（插值）的概念，转换值的定义，就可以进行微分了

![image-20221021165335991](https://gitee.com/dorothea817/pics/raw/master/image-20221021165335991.png)

## Self-attention

应用场景：解决输入为向量集合（Vector  Set）的情况

缺点：**运算量很大**，解决这个问题也是现在的研究趋势

***word Embedding***：将词汇以具有语义的向量形式表示出来

![image-20221021170556452](https://gitee.com/dorothea817/pics/raw/master/image-20221021170556452.png)

数据为向量集合类型的比如：语音、图、分子

输出数据的情况：

- 每个向量都有个label 比如词性标注、语音识别音标、图推荐系统
- 整个向量集合有一个label，比如文本情感分析、语音识别说话的人、分子的特性
- 机器自己决定输出多少个label（seq2seq），比如翻译、语音辨识

*Self-attention 接收一个向量数为n集合，可以生成结合了集合顺序的n个向量，解决了直接将向量分别输入全连接网络后失去顺序特性的问题。*

### 运行原理

![image-20221021172525734](https://gitee.com/dorothea817/pics/raw/master/image-20221021172525734.png)

Self-attention 在接收一个向量数为n集合之后，会分别计算a1-an，每一个其他数据对ai的关联程度α，只去关注关联性高的aj处理后产生输出bi，这个α也称为**attention 注意力**

> **计算α的方法举例**：
>
> - ==Dot-product（点乘）==
>
>   ![image-20221021172649134](https://gitee.com/dorothea817/pics/raw/master/image-20221021172649134.png)
>
> - additive
>
>   ![image-20221021172735802](https://gitee.com/dorothea817/pics/raw/master/image-20221021172735802.png)

![image-20221021172936561](https://gitee.com/dorothea817/pics/raw/master/image-20221021172936561.png)

![image-20221021173116480](https://gitee.com/dorothea817/pics/raw/master/image-20221021173116480.png)

**b1-bn是并行计算的**

 ![image-20221021173901090](https://gitee.com/dorothea817/pics/raw/master/image-20221021173901090.png)

 *Multi-head Self-attention （处理不同种类的相关性）*

![image-20221021174115581](https://gitee.com/dorothea817/pics/raw/master/image-20221021174115581.png)

![image-20221021174131047](https://gitee.com/dorothea817/pics/raw/master/image-20221021174131047.png)

但是这个过程失去了向量在集合的位置信息，可以通过**Positional Encoding**来加入位置信息，将位置设置成位置向量，加上ai即可

### Truncated Self-attention（小范围自注意力机制）

适用于语音识别等向量序列长度特别长方面的Self-attention，可以通过缩小Self-attention的范围来降低运算规模

![image-20221021174942559](https://gitee.com/dorothea817/pics/raw/master/image-20221021174942559.png)

Self-attention也可以用在图像处理上

![image-20221021175059255](https://gitee.com/dorothea817/pics/raw/master/image-20221021175059255.png)

CNN里面的Receptive field的规模可以通过Self-attention学习来，因此，CNN其实就是设定好参数的Self-attention

![image-20221026130708423](https://gitee.com/dorothea817/pics/raw/master/image-20221026130708423.png)

CNN往往在参数量比较小的时候表现比较好，而Self-attention在参数量比较多的时候表现较好

**Self-attention VS RNN**（Recurrent Neural Network 循环神经网络）

> 循环神经网络（Recurrent Neural Network, RNN）是一类以[序列](https://baike.baidu.com/item/序列/1302588?fromModule=lemma_inlink)（sequence）数据为输入，在序列的演进方向进行[递归](https://baike.baidu.com/item/递归/1740695?fromModule=lemma_inlink)（recursion）且所有节点（循环单元）按链式连接的[递归神经网络](https://baike.baidu.com/item/递归神经网络/16020230?fromModule=lemma_inlink)（recursive neural network）。
>
> 其中双向循环神经网络（Bidirectional RNN, Bi-RNN）和长短期记忆网络（Long Short-Term Memory networks，[LSTM](https://baike.baidu.com/item/LSTM/17541102?fromModule=lemma_inlink)）是常见的循环神经网络。
>
> 循环神经网络具有记忆性、参数共享并且[图灵完备](https://baike.baidu.com/item/图灵完备/4634934?fromModule=lemma_inlink)（Turing completeness），因此在对序列的[非线性](https://baike.baidu.com/item/非线性/7127824?fromModule=lemma_inlink)特征进行学习时具有一定优势 [4] 。循环神经网络在[自然语言处理](https://baike.baidu.com/item/自然语言处理/365730?fromModule=lemma_inlink)（Natural Language Processing, NLP），例如[语音识别](https://baike.baidu.com/item/语音识别/10927133?fromModule=lemma_inlink)、语言建模、[机器翻译](https://baike.baidu.com/item/机器翻译/411793?fromModule=lemma_inlink)等领域有应用，也被用于各类[时间序列](https://baike.baidu.com/item/时间序列/1389644?fromModule=lemma_inlink)预报。引入了[卷积神经网络](https://baike.baidu.com/item/卷积神经网络/17541100?fromModule=lemma_inlink)（Convolutional Neural Network,CNN）构筑的循环神经网络可以处理包含序列输入的[计算机视觉](https://baike.baidu.com/item/计算机视觉/2803351?fromModule=lemma_inlink)问题。

RNN没办法**平行处理所有的输出**，但是Self-attention可以做到

![image-20221026131645096](https://gitee.com/dorothea817/pics/raw/master/image-20221026131645096.png)

**Self-attention for Graph**：注意力矩阵内只需要计算出每个edge的值，也就是只有和别的向量具有关联信息才有注意的意义，也是一种**Graph Neural  Network（GNN）**

## Recurrent Neural Network （RNN）

在处理序列向量时，具有**记忆性**，可以将之前得出的output存在**memory**里，之后的每个隐藏层在计算的时候不仅会考虑input，还会考虑memory里面的数据，就可以解决，输入的值一样，但是因为位置不同，输出也应该不同的问题

![image-20221026141346003](https://gitee.com/dorothea817/pics/raw/master/image-20221026141346003.png)

 以上是**Elman Network**的过程，还有**Jordan Network**，它是把每一次整个网络循环的输出存到memory里，然后加到下一次循环的输入里，必Elman Network效果会更好一点

![image-20221026141627222](https://gitee.com/dorothea817/pics/raw/master/image-20221026141627222.png)

### Bidirectional RNN （双向循环神经网络）

解决了在处理序列向量的时候只会收到前面向量影响，而不**考虑后面向量**影响的问题

![image-20221026141853070](https://gitee.com/dorothea817/pics/raw/master/image-20221026141853070.png)

### Long Short-term Memory LSTM  （长短期记忆神经网络）

LSTM分memory部分新加入了三个部分：

- Input Gate 通过信号控制Neuron是否能存到memory里面
- Output Gate 通过信号控制Neuron是否能够取memory里面的值
- Forget Gate 通过信号控制是否清空memory

这三个部分联合起来的结构加上外界输入，一共需要四个输入，最后会产生一个输出

![image-20221026142337775](https://gitee.com/dorothea817/pics/raw/master/image-20221026142337775.png)

***LSTM执行过程***

![image-20221026151409449](https://gitee.com/dorothea817/pics/raw/master/image-20221026151409449.png)

![image-20221026151511198](https://gitee.com/dorothea817/pics/raw/master/image-20221026151511198.png)

 LSTM需要的参数量是普通的NN的参数量的4倍

**完整的LSTM结构图**

![image-20221026153034997](https://gitee.com/dorothea817/pics/raw/master/image-20221026153034997.png)

类型：LSTM、GRU（少了一个gate和1/3的参数，过拟合的概率会更低，input gate 和 forget gate 联动，共同变化）、SimpleRNN

### RNN的训练

RNN的error surface 要么非常陡峭，要么非常平坦，所以训练的结果不好（梯度爆炸）

![image-20221026162937563](https://gitee.com/dorothea817/pics/raw/master/image-20221026162937563.png)

**解决方法**

- 可以设置一个阈值Clipping，使gradient>这个阈值的时候，就直接等于这个阈值，避免了大跨步的现象（梯度裁剪）

- 也可以使用LSTM，可以使梯度消亡，避免梯度爆炸

  RNN的memory参数是相乘关系，并且会把之前存过得memory直接覆盖掉，而LSTM是相加的关系，并且之前存下来的memory会一直对之后的结果产生影响

- Clockwise RNN

- Structurally Constrained Recurrent network（SCRN）

### 应用

***输入x的长度和输出y的长度不一致的情况***

- many to one （文本情感分析 ）

  ![image-20221026164559054](https://gitee.com/dorothea817/pics/raw/master/image-20221026164559054.png)

- many to many （output < input） 比如语音辨识

  ![image-20221026164819900](https://gitee.com/dorothea817/pics/raw/master/image-20221026164819900.png)

  解决方法 **CTC**，补∅

  > CTC，基于[神经网络](https://so.csdn.net/so/search?q=神经网络&spm=1001.2101.3001.7020)的时序类分类，[语音识别](https://so.csdn.net/so/search?q=语音识别&spm=1001.2101.3001.7020)声学模型的训练属于监督学习，需要知道每一帧对应的label才能进行有效的训练，在训练的数据准备阶段必须要对语音进行强制对齐。
  > CTC的引入可以放宽了这种一一对应的限制要求，只需要一个输入序列和一个输出序列即可以训练。有两点好处：不需要对数据对齐和一一标注；CTC直接输出序列预测的概率，不需要外部的后处理。

  ![image-20221026164903253](https://gitee.com/dorothea817/pics/raw/master/image-20221026164903253.png)

- Many to Many (不确定哪个比较长) 比如机器翻译

  ![image-20221026165607300](https://gitee.com/dorothea817/pics/raw/master/image-20221026165607300.png)

- Beyond Sequence （句法分析，构建句法树）

  ![image-20221026165922498](https://gitee.com/dorothea817/pics/raw/master/image-20221026165922498.png)

- Sequence to  Sequence Auto-encoder-Speech

  例如在英语语音翻译成中文的时候，之前的做法是先把语音识别成英文文字，然后在对英文文字进行翻译，Auto-encoder-Speech 是收集对应的英语语音的中文翻译，直接去分析语音，然后得出对应的中文翻译

-  Attention-based Model，有选择的提取数据

  ![image-20221026171910854](https://gitee.com/dorothea817/pics/raw/master/image-20221026171910854.png)

   **Attention-based Model V2 （Neural Turing Machine 神经图灵机）**，不仅实现了V1的功能，还能有选择的去存数据

  ![image-20221026172027100](https://gitee.com/dorothea817/pics/raw/master/image-20221026172027100.png)

- Visual Question Answer

  ![image-20221026172326083](https://gitee.com/dorothea817/pics/raw/master/image-20221026172326083.png)

- Speech Question Answer

  ![image-20221026172429500](https://gitee.com/dorothea817/pics/raw/master/image-20221026172429500.png)

## Graph Neural Networks （GNN）图神经网络

> ***图（Graph）***
>
> 在讨论GNN之前，我们先来了解一下什么是**图**。在计算机科学中，**图是由节点和边两部分组成的一种数据结构**。图G可以通过节点集合V和它包含的边E来进行描述。如下图所示：
>
> ![img](https://img-blog.csdnimg.cn/6f573e8c8bc346eaac130e72315d1ec5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGf5Y2X57a_6Zuo,size_20,color_FFFFFF,t_70,g_se,x_16)
>
> GNN全称----图神经网络，它是一种**直接作用于图结构**上的神经网络。我们可以把图中的每一个节点 V 当作个体对象，而每一条边 E 当作个体与个体间的某种联系，所有节点组成的关系网就是最后的图 U 。
> 这里的V , E , U 都可以编码成一个**特征向量**，所以实际上GNN还是做的是**提取特征**的工作而已。GNN的一个典型应用是**节点分类**，我们希望利用GNN提取出每个节点 V 的特征向量，来预测每个节点的标签。同样的，也可以通过节点与节点间的特征，来预测出对应边 E的标签。当然，也可以利用所以节点提取出的特征，来预测整个图 V 的标签。 如下图：
>
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/f1ed421594464aa9ad649a6ba4be29f0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGf5Y2X57a_6Zuo,size_20,color_FFFFFF,t_70,g_se,x_16)
>
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/23c7e2e186374f4e90123aef84eff3b5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGf5Y2X57a_6Zuo,size_20,color_FFFFFF,t_70,g_se,x_16)

### 应用

- 分类 Classification，例如分子分类

  ![image-20221027124556981](https://gitee.com/dorothea817/pics/raw/master/image-20221027124556981.png)

- 生成 Generation，例如分子生成

  ![image-20221027124710269](https://gitee.com/dorothea817/pics/raw/master/image-20221027124710269.png)

- 处理复杂的数据间关系

### Spatial-based 基于空间的GNN

> **聚合（Aggregate）**：用邻居特性更新下一层的隐藏状态
>
> **读出（Readout）**：把所有节点的特征集合起来代表整个图
>
> ![image-20221027141210775](https://gitee.com/dorothea817/pics/raw/master/image-20221027141210775.png)

#### NN4G（Neural Networks for Graph）

先给图中每个结点设定相应的特征值x~n~，然后经过embedding，用一个embedding矩阵W1 乘以对应的特征值x~n~ 得到 h~n~^0^，第0层的隐藏层上的值，然后下一层的隐藏层的值需要加上当前结点所有的邻居的值，乘以权重后加上embedding的值，这就是NN4G的聚合过程。

> **什么是Embedding？**
>
> Embedding（嵌入）是拓扑学里面的词，在深度学习领域经常和Manifold（流形）搭配使用。
>
> 可以用几个例子来说明，比如三维空间的球体是一个二维流形嵌入在三维空间（2D manifold embedded in 3D space）。之所以说他是一个二维流形，是因为球上的任意一个点只需要用一个二维的经纬度来表达就可以了。
>
> 又比如一个二维空间的旋转矩阵是2x2的矩阵，其实只需要一个角度就能表达了，这就是一个一维流形嵌入在2x2的矩阵空间。
>
> **什么是深度学习里的Embedding？**
>
> 这个概念在深度学习领域最原初的切入点是所谓的**Manifold Hypothesis**（流形假设）。流形假设是指“自然的原始数据是低维的流形嵌入于(embedded in)原始数据所在的高维空间”。那么，**深度学习的任务就是把高维原始数据（图像，句子）映射到低维流形**，使得高维的原始数据被映射到低维流形之后变得可分，而这个映射就叫嵌入（Embedding）。比如Word Embedding，就是把单词组成的句子映射到一个表征向量。但后来不知咋回事，开始把低维流形的表征向量叫做Embedding，其实是一种误用。。。
>
> 如果按照现在深度学习界通用的理解（其实是偏离了原意的），**Embedding就是从原始数据提取出来的Feature，也就是那个通过神经网络映射之后的低维向量**。也就是特征向量Feature vector

![image-20221027141714512](https://gitee.com/dorothea817/pics/raw/master/image-20221027141714512.png)

当GNN叠了很多层的时候，我们要想找出整个图的特征，需要做readout，NN4G里面把每一层的所有节点特征整合起来得出X~n~，然后各自经过一个transform在加起来变成一个feature

![image-20221027142933245](https://gitee.com/dorothea817/pics/raw/master/image-20221027142933245.png)

#### DCNN（Diffusion-Convolution Neural Network）

扩散卷积神经网络

dcnn的卷积过程：首先给每个结点特征值x~n~，然后上面每一层隐藏层h~n~^m^的值为在图上，和第n个结点之间的距离为m+1的节点的特征值相加，然后在经过权重transform

![image-20221027150553160](https://gitee.com/dorothea817/pics/raw/master/image-20221027150553160.png)

然后每一个隐藏层的所有节点的值合起来得到矩阵Hm，把所有的H叠在一起，如果想要做节点分类的话，可以取出单独的一个节点的向量，经过一个transform 得出这个节点的特征值。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200910200229265.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTM4OTAwMQ==,size_16,color_FFFFFF,t_70#pic_center)

#### DGC（Diffusion Graph Convolution）

扩散图卷积

和DCNN相比的区别 只是把得出的H~m~ 加起来，而不是叠起来

![image-20221027151504273](https://gitee.com/dorothea817/pics/raw/master/image-20221027151504273.png)

#### MoNET（Mixture Model Networks）

混合模型网络，他在Aggregate的过程中，并不像以上方式的简单相加，而是考虑了距离作为权重

![image-20221027151802101](https://gitee.com/dorothea817/pics/raw/master/image-20221027151802101.png)

GraphSAGE

> GraphSAGE 是 2017 年提出的一种图神经网络算法，解决了 GCN 网络的局限性: GCN 训练时需要用到整个图的邻接矩阵，依赖于具体的图结构，一般只能用在直推式学习 Transductive Learning。GraphSAGE 使用多层聚合函数，每一层聚合函数会将节点及其邻居的信息聚合在一起得到下一层的特征向量，GraphSAGE 采用了节点的邻域信息，不依赖于全局的图结构。

算法分为三个部分：1）对节点进行[采样](https://so.csdn.net/so/search?q=采样&spm=1001.2101.3001.7020)；2）对节点的邻居进行聚合；3）根据聚合的信息对节点进行学习

聚合的方法有三种：平均、最大池化或者LSTM

**平均**
![MEAN AGGREGATIR](https://img-blog.csdnimg.cn/2020121016204096.png#pic_center)
对当前节点及其邻居节点进行MEAN操作，这里mean可以看作GCN中的卷积操作，是对节点及其邻居向量的局部谱卷积的线性相似。
需要注意，这里的MEAN操作和下面其他两种的平均池化的区别：Mean aggregater 不执行串联操作 即“跳跃连接” 。

**LSTM**

考虑到LSTM是针对序列的模型，不具备排列不变性，因此，这里在对邻居节点进行聚合之前，对所有邻居进行随机排列。

**最大池化**

![池化聚合](https://img-blog.csdnimg.cn/20201210162828997.png)
现将所有邻居的向量放进一个全连接网络，后接上一个最大池化层，或者平均池化层。理论上，池化层之前可以接任意层的神经网络，但是为了简化模型，本论文中使用的是单层的神经网络。不管是平均池化或者最大池化，得到的结果都差不多。

#### GAT（Graph Attention Networks）

图注意力网络

> GAT 是一种使用了self attention机制图神经网络，该网络使用类似transformer里面self attention的方式计算图里面某个节点相对于每个邻接节点的注意力，将节点本身的特征和注意力特征concate起来作为该节点的特征，在此基础上进行节点的分类等任务。

GAT的聚合过程：在计算下一层的隐藏层的值时，用weight sum ，但是不像MoNET，这个权重是自己设定的，要让机器自己去学，对节点的邻居做attention。定义了一个参数叫做energy，代表了对于当前节点来说，每个邻居分别有多重要，这个energy就作为当前邻居节点的权重，然后通过weight sum 计算下一层。

![image-20221027154509558](https://gitee.com/dorothea817/pics/raw/master/image-20221027154509558.png)

#### GIN（Graph Isomorphism Network）

图同构网络

对于一些GNN方法的理论证明，结论表示，在进行update隐藏层的时候 要把邻居结点都加起来，然后再加上某一个concept （可无）乘以他自己的值，重点是要用相加的方法，而不是绝对值池化或者最大值池化。

![image-20221027155547741](https://gitee.com/dorothea817/pics/raw/master/image-20221027155547741.png)

### Spectral-based 基于光谱的GNN

普通的CNN在update下一个隐藏层的时候，会通过卷积核，逐步计算一个矩阵，内积计算得出相应的特征矩阵，但是在图神经网络中，输入是有结构的图，不能和卷积核进行内积的计算，因此考虑把一层的所有节点的**信号**，再将卷积核进行傅里叶变换，然后**相乘**再进行傅里叶逆变换。就能得出下一层的信号。

![image-20221027163128826](https://gitee.com/dorothea817/pics/raw/master/image-20221027163128826.png)

**信号的计算过程：**

![image-20221027165703795](https://gitee.com/dorothea817/pics/raw/master/image-20221027165703795.png)

**filter的计算过程**

![image-20221027165915321](https://gitee.com/dorothea817/pics/raw/master/image-20221027165915321.png)

**下一层信号的计算方式**

![image-20221027170015284](https://gitee.com/dorothea817/pics/raw/master/image-20221027170015284.png)

![image-20221027170203594](https://gitee.com/dorothea817/pics/raw/master/image-20221027170203594.png)

存在问题：

- 计算复杂度高*O(N)*
- 无法保证局部链接

####  ChebNet

重点：**采用切比雪夫多项式代替谱域的卷积核**

优点：快且可以做到局部化

![image-20221027172046205](https://gitee.com/dorothea817/pics/raw/master/image-20221027172046205.png)

为了解决问题三，引入切比雪夫多项式来计算下面这个式子：

![image-20221027172350625](https://gitee.com/dorothea817/pics/raw/master/image-20221027172350625.png)

最后式子展开为：

![image-20221027173545219](https://gitee.com/dorothea817/pics/raw/master/image-20221027173545219.png)

#### GCN（Graph Convolution Network）

图卷积网络

> GCN，图卷积神经网络，实际上跟CNN的作用一样，就是一个特征提取器，只不过它的对象是图数据。GCN精妙地设计了一种从图数据中提取特征的方法，从而让我们可以使用这些特征去对图数据进行节点分类（node classification）、图分类（graph classification）、边预测（link prediction），还可以顺便得到图的嵌入表示（graph embedding）。

![image-20221027174053340](https://gitee.com/dorothea817/pics/raw/master/image-20221027174053340.png)

![image-20221027174748232](https://gitee.com/dorothea817/pics/raw/master/image-20221027174748232.png)

问题：再叠多层的GCN后，效果会变差，同一个网络的结点可能会收敛到同一个地方

![image-20221027175455400](https://gitee.com/dorothea817/pics/raw/master/image-20221027175455400.png)

解决方法：在对edge取平均的时候drop掉一些

![image-20221027175539356](https://gitee.com/dorothea817/pics/raw/master/image-20221027175539356.png)

#### Graph Generation

生成图

- VAE-based model: Generate a whole graph in one step 一步生成整张图

  ![image-20221027175920648](https://gitee.com/dorothea817/pics/raw/master/image-20221027175920648.png)

- GAN-based model: Generate a whole graph in one step 一步生成整张图

  ![image-20221027180104701](https://gitee.com/dorothea817/pics/raw/master/image-20221027180104701.png)

- Auto-regressive-based model: Generate a node or an edge in one step 一步生成一个结点或者一个边

  ![image-20221027180127305](https://gitee.com/dorothea817/pics/raw/master/image-20221027180127305.png)
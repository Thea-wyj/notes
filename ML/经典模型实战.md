# 模型实战

## 环境配置

### 安装Anaconda

下载Anaconda www.anaconda.com

![image-20220923101714883](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220923101714883.png)

### 换源

```
//清华源
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --set show_channel_urls yes
//中科大源
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/
conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/
conda config --set show_channel_urls yes
//恢复默认
conda config --remove-key channels
//查看源
conda config --show
```

### 修改环境默认地址

1、使用 `conda config --show` 查看envs_dirs的信息，一般第一个路径是默认路径。把它修改成想要自定义的路径

2、在 .condarc文件内加入以下内容

  - ```
    envs_dirs:
    
    - D:\SDE\Anaconda\Anaconda3\envs

### 配置不同版本的PyTorch运行环境

 在Anaconda Prompt 命令行运行

`conda create -n [环境名] python=[环境版本号]`

![image-20220923102316838](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220923102316838.png)

y

**激活环境**

`conda activate [环境名]`

### 安装PyTorch

> 如果电脑上没有cuda的话，需要先安装cuda
>
> 1. 在NVIDA 控制面板 查看自己驱动的版本 ：帮助 》系统信息 》组件
>
>    ![image-20230713215649349](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230713215649349.png)
>
> 2. 在cuda[下载网站](https://developer.nvidia.cn/cuda-toolkit-archive)下载对应的工具包
>
>    ![image-20230713215945122](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230713215945122.png)
>
>    ![image-20230713220016917](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230713220016917.png)
>
> 3. 完成安装后可以在cmd使用`nvcc -V` 命令行检测是否安装完成
>
>    ![image-20230713222133866](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230713222133866.png)
>
> 4. 安装对应的[CuDNN](https://developer.nvidia.com/rdp/cudnn-archive)
>
>    下载对应CUDA版本的CuDNN
>    下载完成后，解压得到一个文件夹，将该文件夹下的文件复制到上一步安装的CUDA中，注意对应的文件夹；
>    ./cuda/bin/.dll 复制到 ./NVIDIA GPU Computing Tookit/CUDA/v11.4/bin/
>    ./cuda/include/.h 复制到 ./NVIDIA GPU Computing Tookit/CUDA/v11.4/include/
>    ./cuda/lib/x64/**.dll 复制到 ./NVIDIA GPU Computing Tookit/CUDA/v11.4/lib/x64/
>
> 5. 安装好后，测试Pytorch是否可以使用
>

https://pytorch.org/

![image-20220923102703507](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220923102703507.png)

```
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch
```



- 查看安装包列表 `conda list`

运行完再运行一遍可以检查是否安装成功

`python`

\>\>\>`import torch` (没报错说明成功了)

### 安装Jupyter

`conda install nb_conda`

y

### pycharm配置anaconda环境

1. 下载[pycharm](https://www.jetbrains.com/pycharm/)

## pycharm配置服务器环境

1. 下载[Xshell](https://www.xshell.com/zh/xshell/)和[Xftp](https://www.xshell.com/zh/xftp/)

2. 配置服务器

    > 实验室服务器ip：10.105.240.33  端口号：22
    >
    > 用户名：dorothea  密码：032526
    >
    > 实验室管理员账号：user  密码：bupt212admin

    ![image-20230717113702928](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230717113702928.png)

3. pycharm配置服务器

    ![image-20230717114135644](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230717114135644.png)

    ![image-20230717114209605](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230717114209605.png)

    ![image-20230717114330387](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230717114330387.png)

    ![image-20230717114450275](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230717114450275.png)

    ![image-20230717115314153](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230717115314153.png)

    ![image-20230717115349743](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230717115349743.png)

4. 新建服务器python编译器

    通过xshell安装anaconda，并新建环境

    > 详情见 [环境配置](#环境配置)

    在pycharm上新建ssh编译器

    ![image-20230717125549329](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230717125549329.png)

    ![image-20230717130045736](https://raw.githubusercontent.com/Thea-wyj/pic/main/image-20230717130045736.png)



## Pytorch

一个Python里面用于**机器学习的框架**

功能：

- 可以用**GPU来加速**高维矩阵（Tensor）的**运算**
- 可以进行深度神经网络中的**自动微分**

![image-20220927150330055](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927150330055.png)

### 加载数据

![image-20220927150812950](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927150812950.png)

**Dataset**： 存储数据样本并获得期望值

dataset = MyDataset(file)

dataloader = DataLoader(dataset,batch_size,shuffle=True)   【Training：True；Testing：False】

**Dataloader**：将数据分组，使之能够同步处理

#### Tensor

Pytorch中的高维矩阵

![image-20220927151652538](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927151652538.png)

**获取矩阵每个维度上的长度:tensor.shape()**

![image-20220927151917484](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927151917484.png)

```python
import torch

x = torch.tensor([[1, -1], [2, -2]])
print(x.shape)
print(x.size())
```

**创建Tensors**

- 直接从数据转化：**x = torch.tensor([[1,-1],[-1,1]])**
- 创建0、1矩阵 **x = torch.zeros([2,2])  x = torch.ones([1,2,5])**

![image-20220927152418697](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927152418697.png)

**操作**

- 简单的加减乘除

  - z = x+y

  - y = x.sum()

  - z = x-y

  - y = x.mean()

  - y=x.pow(2)

- 转置  x = x.transpose(0,1)

  ![image-20220927152939608](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927152939608.png)

- 压缩（将tensor中大小为1的维度删除，对特定[维度](https://so.csdn.net/so/search?q=维度&spm=1001.2101.3001.7020)进行squeeze，如果该维度大小不为1，则保持原来的shape不变） b=torch.squeeze(a)

  ![image-20220927153122918](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927153122918.png)

- 升维 给指定位置加上维数为一的维度  x=x.unsqueeze(1)

  ![image-20220927153212904](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927153212904.png)

- 合并矩阵 w = torch.cat([x,y,z],dim =1)

  ![image-20220927153322179](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927153322179.png)

Tensors中的数据类型

![image-20220927170150077](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927170150077.png)

**更改运算位置**

![image-20220927170251340](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927170251340.png)

**Tensor计算梯度**

![image-20220927170500914](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927170500914.png)

### 训练神经网络

- 定义神经网络：有哪些层，长什么样子
- 选损失函数
- 选最佳化算法

#### 定义神经网络

nn.Linear(in_features,outfeatures)

![image-20220927172156705](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927172156705.png)

```python
import torch.nn

layer = torch.nn.Linear(32,64)
print(layer.weight.shape)
print(layer.bias.shape)
```

![image-20220927172543940](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927172543940.png)

其他的非线性的激活函数

nn.Sigmoid()

```python
import torch.nn

t1 = torch.randn(3, 3)
layer = torch.nn.Sigmoid()
t2 = layer(t1)
print(t1)
print(t2)
```

![image-20220927173209631](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927173209631.png)

nn.ReLU()

```python
import torch.nn

t1 = torch.randn(3, 3)
layer = torch.nn.ReLU()
t2 = layer(t1)
print(t1)
print(t2)
```

![image-20220927173306351](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927173306351.png)

![image-20220927173446565](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927173446565.png)

```python
import torch.nn as nn


class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(10, 32),
            nn.Sigmoid(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.net(x)
```

#### 定义损失函数

criterion = nn.**MSELoss**() 

**均方损失函数** ![image-20220927174315972](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927174315972.png)

-  如果 reduce = False，那么 size_average 参数失效，直接返回向量形式的 loss
- 如果 reduce = True，那么 loss 返回的是标量
- a)如果 size_average = True，返回 loss.mean();
  b)如果 size_average = False，返回 loss.sum();
- 默认情况下， reduce = True，size_average = True

criterion  = nn.**CrossEntropyLoss**()

**交叉熵损失函数** ：交叉熵是用来度量**两个概率分布的差异性**的，用来衡量模型**学习到的分布和真实分布的差异**。

> ex：关于交叉熵的一些概念
>
> 1. 信息量
>        信息量表示一条信息消除不确定性的程度，如中国目前的高铁技术世界第一，这个概率为1，这句话本身是确定的，没有消除任何不确定性。而中国的高铁技术将一直保持世界第一，这句话是个不确定事件，包含的信息量就比较大。**信息量的大小和事件发生的概率成反比**。
>
>    ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019105319889.png#pic_center)
>
> 2. 信息熵
>
>    信息熵则是在结果出来之前对可能产生的信息量的期望，期望可以理解为所有可能结果的概率乘以该对应的结果。
>
>    ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019125421244.png#pic_center)
>
>     信息熵是用来衡量事物不确定性的。信息熵越大（信息量越大，P越小），事物越具不确定性，事物越复杂。
>
> 3. 相对熵(即KL散度)    
>
>    相对熵又称互熵，设p ( x ) p(x)p(x)和q ( x ) q(x)q(x)是取值的两个概率分布，相对熵用来表示两个概率分布的差异，当两个随机分布相同时，它们的相对熵为零，当两个随机分布的差别增大时，它们的相对熵也会增大。：
>
>    ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019124521788.png#pic_center)
>
> 4. 交叉熵
>
>     将相对熵展开可以得到如下：
>
>    ![image-20220927175147246](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20220927175147246.png)
>
>    由上式可知，相对熵=交叉熵-信息熵，H(p,q)就是交叉熵：
>
>    ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019131526518.png#pic_center)
>
>    由于在机器学习和深度学习中，样本和标签已知（即p已知），那么信息熵H（p）相当于常量，此时，只需拟合交叉熵，使交叉熵拟合为0即可。
>
> 扩展到多分类
>
> 多分类和二分类类似，二分类标签为1和0，而多分类可以用one-hot编码来表示，即现在要预测西瓜品种，品种有黑美人，特小凤，安农二号，如果真实标签为特小凤即（0,1,0），预测标签为安龙二号即（0,0,1），将预测标签的各个概率带入即可，多分类的情况实际上就是对二分类的扩展：
>
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019135058422.png#pic_center)
>
> y~ik~表示第 i 个样本的真实标签为k ，共有K 个标签值, N个样本， p~i,k~表示第 i 个样本预测为第 k 个标签值的概率。通过对该损失函数的拟合，也在一定程度上增大了类间距离。

loss = criterion(model_output,expected_value)

#### 选择优化算法

基于梯度的优化算法 是用来调整网络参数来减小误差的

eg：Stochastic Gradient Descent（SGD）

optimizer = torch.optim.SGD(model.parameters(),lr,momentum=0)

**步骤**

1. 调用optimizer.zero_grad() 重置模型参数的梯度
2. 调用loss.backward()  反向传预测损失的梯度
3. 调用optimizer.step() 来调整模型参数

#### 开始训练

```python
import torch
import torch.nn as nn

dataset = MyDataset(file)  # 通过MyDataset 读取数据 存储数据样本并获得期望值
tr_set = DataLoader(dataset, 16, shffle=True)  # 将数据集放进DataLoader里面 将数据分组，使之能够同步处理
model = MyModel().to(device)  # 创建模型并将模型放到指定的设备上跑
criterion = nn.MSELoss()  # 设定损失函数
optimizer = torch.optim.SGD(model.parameters(), 0.1)  # 设置优化算法

for epoch in range(n_epochs):  # 迭代n_epochs次
    model.train()  # 将模型设置为训练模式
    for x, y in tr_set:  # 通过dataloader进行迭代
        optimizer.zero_grad()  # 将之前步骤得出的梯度清空
        x, y = x.to(device), y.to(device)  # 将data移动到对应的设备
        pred = model(x)  # 计算输出
        los = criterion(pred, y)  # 计算损失
        loss.backward()  # 反向传播计算梯度
        optimizer.step()  # 通过优化算法更新模型参数
```



### 验证神经网络

```python
# 验证模型
model.eval()  # 将模型调到验证模式
total_loss = 0
for x, y in dv_set:  # 通过数据集进行迭代
    x, y = x.to(device), y.to(device)
    with torch.no_grad():  # 关闭梯度的计算
        pred = model(x)  # 计算输出
        loss = criterion(pred, y)  # 计算损失
    total_loss += loss.cpu().item() * len(x)  # 累计损失
    avd_loss = total_loss / len(dv_set.dataset)  # 计算平均损失
```

### 测试神经网络

```python
# 模型测试
model.eval()  # 将模型调到测试模式
preds = []
for x in tt_set  # 通过数据集进行迭代
    x = x.to(device)  # 将数据移到对应设备上
    with torch.no_grad():  # 关闭梯度计算
        pred = model(x)  # 计算输出
        preds.append(pred.cpu())  # 收集预测值
```

**存储模型**

torch.save(model.stata_dict(),path)

**加载模型**

ckpt = torch.load(path)

model.load_state_dict(ckpt)

## 机器学习指南

训练好模型，得出测试数据的结果之后↓

1. 先检查训练数据上的误差是否大

   1. 训练数据误差大

      - model bias 模型太简单，使模型更加复杂一点，比如增加更多的特征、通过深度学习来增加层数 

        ![image-20221006213130232](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20221006213130232.png)

      - 最佳化做的不够好，没有找到Loss最小的参数取值，有可能梯度下降法取值到局部最小值点或者鞍点（saddle point）【统称为**critical point** 临界点】，也有可能是**Learning Rate**的问题【大部分情况下】

      - ==最常用的最佳化算法：Adam （RMSProp + Momentum）==
   
        ![image-20221014151710965](https://gitee.com/dorothea817/pics/raw/master/image-20221014151710965.png)
        
        ![image-20221006213432899](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20221006213432899.png)
        
        > 当取值到局部最小值点的时候是很难进行逃出的，但是鞍点是可以逃出的，需要通过泰勒级数展开和海塞矩阵来近似出Loss的图像
        >
        > **方法一：海塞矩阵求特征向量**
        >
        > ![image-20221007091844852](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20221007091844852.png)
        >
        > 如果海塞矩阵里面的特征值都是正的，则该点是一个局部最小值点，如果都是负的，则是局部最大值点，如果有正有负，那么就是个鞍点
        >
        > ![image-20221007094123277](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20221007094123277.png)
        >
        > 如果是鞍点的话，可以继续向海塞矩阵的特征向量方向进行更新，继续寻找局部最小值点【运算量太大，不推荐】
        >
        > ![image-20221007095009918](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20221007095009918.png)
        >
        > ![image-20221007094900458](https://raw.githubusercontent.com/Thea-wyj/pic/main/img/image-20221007094900458.png)
        >
        > **方法二 Batch**
        >
        > 在训练中用于优化算法的，将数据分成n个batch，每次用一个batch去算计算微分，然后更新参数，完成n个batch的过程称为一个**epoch**，在每一个epoch计算之前，会进行**shuffle**，重新分batch，每个epoch里面的batch分法都不一样，batch size是一个超参数，需要我们自己去调
        >
        > ![image-20221012113936553](https://gitee.com/dorothea817/pics/raw/master/image-20221012113936553.png)
        >
        > *大Batch vs 小 Batch*
        >
        > 大Batch在一定程度上，比小Batch需要的时间要短（GPU平行运算），但是小Batch比大Batch的训练和测试效果要更好
        >
        > ![image-20221012115510358](https://gitee.com/dorothea817/pics/raw/master/image-20221012115510358.png)
        >
        > *总结*
        >
        > ![image-20221012115600130](https://gitee.com/dorothea817/pics/raw/master/image-20221012115600130.png)
        >
        > **方法三 动量（Momentum）**
        >
        > 参考在物理上，如果一个小球从高处斜坡落下，在惯性的影响下，可能不会停在临界点。
        >
        > 一般的梯度下降法：θ^n^ = θ^n-1^ - ηg^n-1^ 
        >
        > 结合动量的梯度下降法：m^0^  = 0   m^n^ = λm^n-1^ -  ηg^n-1^    θ^n^ = θ^n-1^ + m^n^ 
        >
        > 不仅要当前梯度的反方向，还要看上一步前进的方向，相加取一个这种值
        >
        
        *当Learning Rate不合适导致最佳化效果不好时*
        
        > 大的Learning Rate，梯度下降法在遇到比较陡峭的地方，容易发生震荡，容易找不到最小值点，小的Learning Rate在比较平缓的地方，更新的又慢，所以不同的参数，Learning Rate应该不同。
        >
        > ![image-20221013211232161](https://gitee.com/dorothea817/pics/raw/master/image-20221013211232161.png)
        >
        > **σ的算法**
        >
        > 1、Root Mean Square （RMS应用：Adagrad）
        >
        > ![image-20221013211535849](https://gitee.com/dorothea817/pics/raw/master/image-20221013211535849.png)
        >
        > 2、RMSProp
        >
        > 可以区分开最近的梯度和过去的梯度谁对η的影响比较大
        >
        > ![image-20221013211837694](https://gitee.com/dorothea817/pics/raw/master/image-20221013211837694.png)
        >
        > ![image-20221013212125917](https://gitee.com/dorothea817/pics/raw/master/image-20221013212125917.png)
        >
        > ***Learning Rate Scheduling***
        >
        > η和时间有关，时间越长，代表约接近目的地，η就减小
        >
        > ![image-20221013212720747](https://gitee.com/dorothea817/pics/raw/master/image-20221013212720747.png)
        >
        > ***Warm Up***
        >
        > η要先变大，再变小，理解：因为一开始的σ是不准确的，因此先让η经历一个变大又变小的过程有利于σ的确定
        >
        > ![image-20221013213103389](https://gitee.com/dorothea817/pics/raw/master/image-20221013213103389.png)
   
      
   
   2. 训练数据误差小
   
      1. 测试数据上的误差大
         - Overfitting，模型过分拟合训练数据，解决方法：增加训练数据、Data augmentation（数据增强，比如说把图片翻转、放大等，对已有数据做出合理的再加工）、给模型增加限制（更少的神经元、共享参数）、减少特征、提早结束、正则化、Dropout （丢弃一些神经元）
         - mismatch 训练资料和测试资料的分布是不一样的，所以再增加训练资料也不管用
      2. 测试数据上的误差小
         - 结束！

***模型复杂性的权衡（Tradeoff）***

![image-20221014151951290](https://gitee.com/dorothea817/pics/raw/master/image-20221014151951290.png)

## 常用的Optimizers

- SGD

- SGDM（momentum）

- Adgrad

- RMSProp

- Adam（SGDM + RMSProp）

  > **Adam的warm up**
  >
  > RAdam
  >
  > 训练刚开始的时候先用SGDM，训练进行一段时间改用RAdam
  >
  > ![image-20221018155713492](https://gitee.com/dorothea817/pics/raw/master/image-20221018155713492.png)
  >
  > ![image-20221018160131312](https://gitee.com/dorothea817/pics/raw/master/image-20221018160131312.png)


- SWATS

  开始用Adam（快速收敛），然后用SGDM找到平稳的最小值点

  ![image-20221017221925691](https://gitee.com/dorothea817/pics/raw/master/image-20221017221925691.png)

- AMSGrad

  改善Adam容易困在比较陡峭的最小值点内，测试的准确率往往不如训练的准确率

  ![image-20221017222359950](https://gitee.com/dorothea817/pics/raw/master/image-20221017222359950.png)

- AdaBound

  处理比较小的learning rate

  ![image-20221017222606530](https://gitee.com/dorothea817/pics/raw/master/image-20221017222606530.png)

- LR range test

  改善SGDM，调整learning rate，使它处于合适的地方

  ![image-20221018154701407](https://gitee.com/dorothea817/pics/raw/master/image-20221018154701407.png)

- Cyclical LR

  改善SGDM，使learning rate进行周期性的大小变化

  ![image-20221018154708166](https://gitee.com/dorothea817/pics/raw/master/image-20221018154708166.png)

- SGDR

  ![image-20221018154746076](https://gitee.com/dorothea817/pics/raw/master/image-20221018154746076.png)

- One-cycle LR

  warm up + annealing + fine-tuning

  ![image-20221018154824952](https://gitee.com/dorothea817/pics/raw/master/image-20221018154824952.png)

- Lookahead

  可以包裹以上所有的演算法的一个通用的工具，没走K步都会退1步，看一看走的合不合适，避免进入比较陡峭的最小值

  ![image-20221018160840155](https://gitee.com/dorothea817/pics/raw/master/image-20221018160840155.png)

- Nesterov accelerated gradient（NAG）

  超前部署

  ![image-20221018161552825](https://gitee.com/dorothea817/pics/raw/master/image-20221018161552825.png)

- Nadam

  NAG + Adam

  ![image-20221018161648953](https://gitee.com/dorothea817/pics/raw/master/image-20221018161648953.png)

- L2 regularization

  ![image-20221018162208881](https://gitee.com/dorothea817/pics/raw/master/image-20221018162208881.png)

  ![image-20221018162421113](https://gitee.com/dorothea817/pics/raw/master/image-20221018162421113.png)

-  增加演算法的随机性

  - Shuffling  每次的batch重新分
  - Dropout  把不好的输出的neural丢掉
  - Gradient noise  Gradient加上一个高斯noise 

- learning rate的调整

  - warm-up
  - curriculum learning
  - Fine-tuning

- Normalization

- Regularization

***SGD VS Adam***

算法：

![image-20221018164231386](https://gitee.com/dorothea817/pics/raw/master/image-20221018164231386.png)

优缺点：

![image-20221018164304853](https://gitee.com/dorothea817/pics/raw/master/image-20221018164304853.png)

应用：

![image-20221018164452169](https://gitee.com/dorothea817/pics/raw/master/image-20221018164452169.png)